{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker - Bring Your Own Model \n",
    "## Chainer 編\n",
    "\n",
    "ここでは [Chainer](https://chainer.org) のサンプルコードをAmazon SageMaker 上で実行するための移行手順について説明します。SageMaker Python SDK で Chainer を使うための説明は [SDK のドキュメント](https://sagemaker.readthedocs.io/en/stable/using_chainer.html) にも多くの情報があります。\n",
    "\n",
    "注: \n",
    "また、ここでは移行手順の紹介のためトレーニングスクリプトは最小限の書き換えとしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. トレーニングスクリプトの書き換え\n",
    "\n",
    "### 書き換えが必要な理由\n",
    "Amazon SageMaker では、オブジェクトストレージ Amazon S3 をデータ保管に利用します。例えば、S3 上の学習データを指定すると、自動的に  Amazon SageMaker の学習用インスタンスにデータがダウンロードされ、トレーニングスクリプトが実行されます。トレーニングスクリプトを実行した後に、指定したディレクトリにモデルを保存すると、自動的にモデルがS3にアップロードされます。\n",
    "\n",
    "トレーニングスクリプトを SageMaker に持ち込む場合は、以下の点を修正する必要があります。\n",
    "- 学習用インスタンスにダウンロードされた学習データのロード\n",
    "- 学習が完了したときのモデルの保存\n",
    "\n",
    "これらの修正は、トレーニングスクリプトを任意の環境に持ち込む際の修正と変わらないでしょう。例えば、自身のPCに持ち込む場合も、`/home/user/data` のようなディレクトリからデータを読み込んで、`/home/user/model` にモデルを保存したいと考えるかもしれません。同様のことを SageMaker で行う必要があります。\n",
    "\n",
    "### 書き換える前に保存先を決める\n",
    "\n",
    "このハンズオンでは、S3からダウンロードする学習データ・バリデーションデータと、S3にアップロードするモデルは、それぞれ以下のように学習用インスタンスに保存することにします。`/opt/ml/input/data/train/`といったパスに設定することは奇異に感じられるかもしれませんが、これらは環境変数から読み込んで使用することが可能なパスで、コーディングをシンプルにすることができます。[1-1. 環境変数の取得](#env)で読み込み方法を説明します。\n",
    "\n",
    "#### 学習データ\n",
    "- 画像: `/opt/ml/input/data/train/image.npy`\n",
    "- ラベル: `/opt/ml/input/data/train/label.npy`\n",
    "\n",
    "#### バリデーションデータ\n",
    "- 画像: `/opt/ml/input/data/test/image.npy`\n",
    "- ラベル: `/opt/ml/input/data/test/label.npy`\n",
    "\n",
    "#### モデル\n",
    "`/opt/ml/model` 以下にシンボルやパラメータを保存する\n",
    "\n",
    "### 書き換える箇所\n",
    "まず [サンプルのソースコード](https://github.com/chainer/chainer/blob/v5/examples/mnist/train_mnist.py) を以下のコマンドでダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/chainer/chainer/v5/examples/mnist/train_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードされた `train_mnist.py` をファイルブラウザから見つけて開いて下さい (JupyterLab の場合は左右にファイルを並べると作業しやすいです)。あるいはお好きなエディターをお使い頂いても結構です。この`train_mnist.py`は、トレーニングスクリプト内で以下の関数を呼び出し、S3以外からデータをダウンロードしています。\n",
    "\n",
    "```python\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "```\n",
    "\n",
    "こういった方法も可能ですが、今回はS3から学習データをダウンロードして、前述したように`/opt/ml/input/data/train/`といったパスから読み出して使います。書き換える点は主に3点です:\n",
    "\n",
    "1. 環境変数の取得  \n",
    "    SageMaker では、学習データやモデルの保存先はデフォルトで指定されたパスがあり、これらを環境変数から読み込んで使用することが可能です。環境変数を読み込むことで、学習データの位置をトレーニングスクリプト内にハードコーディングする必要がありません。もちろんパスの変更は可能で、API経由で渡すこともできます。\n",
    "    \n",
    "1. 学習データのロード  \n",
    "    環境変数を取得して学習データの保存先がわかれば、その保存先から学習データをロードするようにコードを書き換えましょう。\n",
    "\n",
    "1. 学習済みモデルの保存形式と出力先の変更  \n",
    "    SageMaker では TensorFlow の SavedModel 形式をサポートし、TensorFlow Serving によってデプロイします。もとの`cnn_mnist.py`では、checkpointが保存されるのみでデプロイに十分な情報がありません。SavedModel 形式で保存されるようにコードを追加します。その際、モデルの保存先を正しく指定する必要があります。学習が完了すると学習用インスタンスは削除されますので、保存先を指定のディレクトリに変更して、モデルがS3にアップロードされるようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"env\"></a>1-1. 環境変数の取得\n",
    "\n",
    "Amazon SageMaker の Script Mode では、トレーニングに用いるコードが実行時に Python スクリプトとして実行されます。その際、データ・モデルの入出力は [こちら](https://sagemaker.readthedocs.io/en/stable/using_chainer.html) に記述があるよう `SM_CHANNEL_XXXX` や `SM_MODEL_DIR` という環境変数を参照する必要があります。そのため、`argparse.ArgumentParser` で渡された環境変数と、スクリプト実行時のハイパーパラメータを取得します。\n",
    "\n",
    "![データのやりとり](../img/sagemaker-data-model.png)\n",
    "\n",
    "`SM_CHANNEL_TRAIN`, `SM_CHANNEL_TEST`, `SM_MODEL_DIR` の環境変数の値を取得するよう、以下をトレーニングスクリプトに追加します。ここでは、main関数の中でハイパーパラメータが参照されているので、main関数の中で　`parser = argparse.ArgumentParser()` の次の行にこれらを追加します。下記を実行すると、`SM_OUTPUT_DATA_DIR'` , `SM_CHANNEL_TRAIN` ,`SM_CHANNEL_TEST`, `SM_MODEL_DIR` の３つの環境変数を、`args.train`、`args.test`、`args.model_dir` に格納します。\n",
    "\n",
    "```\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "```\n",
    "\n",
    "これらの値は、create-training-jobのAPIを実行する際に (SageMaker Python SDK で estimator を呼び出す際に) 指定した hyperparameters の値に置き換えることができます。例えば、hyperparameters に `train`、`test`、`model-dir`が指定されていれば、環境変数の値は hyperparameters の値で上書きされます。\n",
    "\n",
    "`train_mnist.py`は、`if __name__ == \"__main__\":`から`main():`を実行します。これで学習データ・バリデーションデータの保存先を取得することができました。次にこれらのファイルを実際に読み込む処理を実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 学習データのロード\n",
    "\n",
    "元のコードでは `chainer.datasets.get_mnist` を利用してダウンロード・読み込みを行っています。具体的には、`main()`のなかにある以下の1行です。今回はS3からデータをダウンロードするため、これらのコードは不要です。**ここで削除しましょう**。\n",
    "```\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    "```\n",
    "\n",
    "代わりにS3からダウンロードしたデータを読み込みコードを実装しましょう。環境変数から取得した `train_dir`や`test_dir` にデータを保存したディレクトリへのパスが保存され、それぞれ `/opt/ml/input/data/train`, `/opt/ml/input/data/test` となります。詳細は [ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-trainingdata) をご覧下さい。\n",
    "\n",
    "今回は npz のファイルを読むようにコードを書き換えれば良いので、以下のようなコードを追記します。パスが `train_dir`, `test_dir` に保存されていることをうまく利用しましょう。もとの npy のデータタイプは uint8 ですが、TensorFlow のレイヤーが対応する float32 や int32 に変換したり、画像の値を 0 から 1 の範囲内になるようにします。\n",
    "```\n",
    "  import os\n",
    "    train_data = np.load(os.path.join(args.train, 'train.npz'))['images']\n",
    "    train_labels = np.load(os.path.join(args.train, 'train.npz'))['labels']\n",
    "\n",
    "    test_data = np.load(os.path.join(args.test, 'test.npz'))['images']\n",
    "    test_labels = np.load(os.path.join(args.test, 'test.npz'))['labels']\n",
    "\n",
    "    train = chainer.datasets.TupleDataset(train_data, train_labels)\n",
    "    test = chainer.datasets.TupleDataset(test_data, test_labels)\n",
    "```\n",
    "\n",
    "#### 確認\n",
    "\n",
    "ここまでの修正で main(unused_argv) の冒頭の実装が以下の様になっていることを確認しましょう。\n",
    "\n",
    "```\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Chainer example: MNIST')\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "    parser.add_argument('--training-steps', type=int, default=20000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 学習済みモデルの保存形式と出力先の変更\n",
    "\n",
    "Chainerでは `model_fn()` を実装することによって、モデルに対して入力を定義することができます。また、冒頭で`import numpy as np`を追加し、以下の関数を、トレーニングスクリプトの末尾に追加します。ここでは、***\n",
    "\n",
    "```\n",
    "def model_fn(model_dir):\n",
    "    chainer.config.train = False\n",
    "    model = chainer.links.Classifier(MLP(1000, 10))\n",
    "    chainer.serializers.load_npz(os.path.join(model_dir, 'model.npz'), model)\n",
    "    return model.predictor\n",
    "```\n",
    "\n",
    "ただし、このままではこの関数は実行されず、SavedModel 形式のモデルは保存されません。main関数の最後でこの関数を呼びつつ、`export_savedmodel`でモデルが保存されるようにします。main 関数の最後は以下のような実装になります。\n",
    "\n",
    "```\n",
    "    # Run the training\n",
    "    trainer.run()\n",
    "    # ... train `model`, then save it to `model_dir` as file 'model.npz'\n",
    "    chainer.serializers.save_npz(os.path.join(args.model_dir, 'model.npz'), model)\n",
    "```\n",
    "ここで、`save_npz` で出力先の `model_dir` を指定することで、学習終了後にモデルが S3 にアップロードされるようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Notebook 上でのデータ準備\n",
    "\n",
    "トレーニングスクリプトの書き換えは終了しました。　学習を始める前に、予め Amazon S3 にデータを準備しておく必要があります。この Notebook を使ってその作業をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習に利用する手書き数字データセットの MNIST を利用します。`keras.datasets`を利用してデータセットをダウンロードし、それぞれ npy 形式で保存します。dataset のテストデータ `(X_test, y_test)` はさらにバリデーションデータとテストデータに分割します。学習データ `X_train, y_train` とバリデーションデータ `X_valid, y_valid` のみを学習に利用するため、これらを npy 形式でまずは保存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを Amazon S3 にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "train_images = np.array([data[0] for data in train])\n",
    "train_labels = np.array([data[1] for data in train])\n",
    "test_images = np.array([data[0] for data in test])\n",
    "test_labels = np.array([data[1] for data in test])\n",
    "\n",
    "try:\n",
    "    os.makedirs('/tmp/data/train')\n",
    "    os.makedirs('/tmp/data/test')\n",
    "\n",
    "    np.savez('/tmp/data/train/train.npz', images=train_images, labels=train_labels)\n",
    "    np.savez('/tmp/data/test/test.npz', images=test_images, labels=test_labels)\n",
    "\n",
    "    train_input = sagemaker_session.upload_data(\n",
    "        path=os.path.join('/tmp/data', 'train'),\n",
    "        key_prefix='notebook/chainer/mnist')\n",
    "    test_input = sagemaker_session.upload_data(\n",
    "        path=os.path.join('/tmp/data', 'test'),\n",
    "        key_prefix='notebook/chainer/mnist')\n",
    "finally:\n",
    "    shutil.rmtree('/tmp/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local Mode によるトレーニングとコードの検証\n",
    "トレーニングジョブを始める前に、Local Mode を使って、この Notebook インスタンス上でコンテナを実行してコードをデバッグしましょう。\n",
    "`from sagemaker.tensorflow import TensorFlow` で読み込んだ SageMaker Python SDK の TensorFlow Estimator を作ります。\n",
    "\n",
    "ここでは、学習に利用するインスタンス数 `train_instance_count` や  インスタンスタイプ `train_instance_type` を指定します。Local modeの場合は、`train_instance_type = \"local\"` と指定します。\n",
    "\n",
    "デバッグなので多くの学習ステップを回す必要はありません。`training-steps` を hyperparameters で渡すことができるようになりましたので、`hyperparameters = {\"training-steps\": 100}` だけ実行するようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.chainer.estimator import Chainer\n",
    "\n",
    "instance_type = \"local\"\n",
    "\n",
    "mnist_estimator = Chainer(entry_point='train_mnist.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type=instance_type,\n",
    "                             framework_version='5.0.0',\n",
    "#                             py_version='py3',\n",
    "#                             hyperparameters = {\"training-steps\": 100}\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`estimator.fit` によりトレーニングを開始しますが、ここで指定する「チャネル」によって、環境変数名 `SM_CHANNEL_XXXX` が決定されます。この例の場合、`'train', 'test'` を指定しているので、`SM_CHANNEL_TRAIN`, `SM_CHANNEL_TEST` となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist_estimator.fit({'train': train_input, 'test': test_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cnn_mnist.py` の中で書き換えを忘れた部分があったら、ここでエラーとなる場合があります。Local Mode ではクイックにデバッグができるので、正しく実行できるよう試行錯誤しましょう。\n",
    "\n",
    " `===== Job Complete =====`\n",
    "と表示されれば成功です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済みモデルの確認\n",
    "\n",
    "Amazon S3 に保存されたモデルは普通にダウンロードして使うこともできます。保存先は `estimator.model_data` で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS CLI を使ってノートブックインスタンス上にモデルをダウンロードして試しに推論します。\n",
    "\n",
    "このノートブックと同じディレクトリに tar.gz の形式でモデルをダウンロードして展開します。展開後のディレクトリ名は数字の羅列 (Unix time) になります。あとでモデルを読み込むため、正規表現を利用して、このフォルダ名を `model_dir` に保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $mnist_estimator.model_data ./\n",
    "!tar -zxvf ./model.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    \n",
    "dir_list = os.listdir(\".\")\n",
    "pattern = \"[0-9]+\"\n",
    "for d in dir_list:\n",
    "    if re.match(pattern,d):\n",
    "        model_dir = d\n",
    "print(\"model is downloaded to ./{}\".format(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータセットからランダムに10枚選んでテストを行います。先ほど保存した`model_dir`からモデルをロードして、テストデータを入力します。ローカルモードでは学習を少ししか実行しなかったため、ほとんど正しい予測はできていないと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 10\n",
    "\n",
    "select_idx = np.random.choice(np.arange(y_test.shape[0]), test_size)\n",
    "test_sample = X_test[select_idx].reshape([test_size,28,28]) * 1./255\n",
    "    \n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.saved_model.loader.load(sess, [\"serve\"], model_dir)\n",
    "    graph = tf.get_default_graph()\n",
    "    predict = sess.run('softmax_tensor:0',feed_dict={'input_layer:0': test_sample})\n",
    "    for i, pred in enumerate(predict):\n",
    "        print(\"Predict: {}, Ground Truth: {}\".format(np.argmax(pred), y_test[select_idx][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. トレーニングジョブの発行\n",
    "\n",
    "推論されていればコードのデバッグは完了です。次に、Amazon SageMaker のトレーニングジョブとしてトレーニングします。データ・モデルの入出力は変わらず S3 なので、`train_instance_type` に `ml.` で始まる SageMaker のインスタンスを指定するだけで実行できます。(リストは[こちら](https://aws.amazon.com/sagemaker/pricing/instance-types/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='cnn_mnist.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type=instance_type,\n",
    "                             framework_version='1.13',\n",
    "                             py_version='py3',\n",
    "                             hyperparameters = {\"training-steps\": 2500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_estimator.fit({'train': train_data, 'test': valid_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "```\n",
    "Billable seconds: <time>\n",
    "```\n",
    "と出力されればトレーニング終了です。これが実際にトレーニングインスタンスが課金される時間となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 推論エンドポイントのデプロイ\n",
    "\n",
    "`mnist_estimator.deploy` で、トレーニングしたモデルを推論エンドポイントとしてデプロイすることができます。これには数分かかります。(`----!` と表示されればデプロイ完了です。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mnist_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 10\n",
    "select_idx = np.random.choice(np.arange(y_test.shape[0]), test_size)\n",
    "test_sample = X_test[select_idx].reshape([test_size,28,28]) * 1./255\n",
    "\n",
    "for i in range(test_size):\n",
    "    result = predictor.predict(test_sample[i])\n",
    "    prediction = result['predictions'][0]['classes']\n",
    "    print(\"Predict: {}, Ground Truth: {}\".format(prediction, y_test[select_idx][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論エンドポイントは立てっぱなしにしているとお金がかかるので、確認が終わったら忘れないうちに削除してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow を使った Amazon SageMaker への移行手順について紹介しました。普段お使いのモデルでも同様の手順で移行が可能ですのでぜひ試してみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p36",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
